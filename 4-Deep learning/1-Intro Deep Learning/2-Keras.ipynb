{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Librería para programar redes neuronales de una manera más sencilla que con TensorFlow. Keras se encuentra en una capa de abstracción por encima de TensorFlow.\n",
    "\n",
    "[Documentación](https://keras.io/guides/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (14.0.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (1.47.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (23.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.16.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: keras in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.9.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow --user\n",
    "#!pip install keras --user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos importando librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos de mnist. No vamos a tratar imagenes con redes convolucionales (perdemos la estructura espacial 2D). Todos los pixeles se convertirán en un vector de 28x28 features independientes, que serán las entradas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cogemos las imágenes de los dígitos asi como el conjunto de train y test\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos dimensiones del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "60.000 imagenes de 28x28 pixeles\n",
    "'''\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60.000 imágenes de 28x28 pixeles. Vamos a representar una de ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_train[0], cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagen se compone de 28x28 pixeles, y cada pixel representa una escala de grises que va del 0 al 255. Siendo 0 el blanco y 255 negro.\n",
    "\n",
    "¿Se te ocurre alguna manera de normalizar los datos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5019607843137255"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "255/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\")/255\n",
    "X_test = X_test.astype(\"float32\")/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13066062"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "        0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764706, 0.14117648,\n",
       "        0.36862746, 0.6039216 , 0.6666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235295, 0.6745098 ,\n",
       "        0.99215686, 0.9490196 , 0.7647059 , 0.2509804 , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19215687, 0.93333334, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.9843137 , 0.3647059 , 0.32156864,\n",
       "        0.32156864, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07058824, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7764706 ,\n",
       "        0.7137255 , 0.96862745, 0.94509804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.3137255 , 0.6117647 ,\n",
       "        0.41960785, 0.99215686, 0.99215686, 0.8039216 , 0.04313726,\n",
       "        0.        , 0.16862746, 0.6039216 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "        0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54509807, 0.99215686, 0.74509805, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313726, 0.74509805, 0.99215686, 0.27450982,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.13725491, 0.94509804, 0.88235295,\n",
       "        0.627451  , 0.42352942, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31764707, 0.9411765 ,\n",
       "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "        0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0627451 , 0.3647059 , 0.9882353 , 0.99215686, 0.73333335,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.9764706 , 0.99215686, 0.9764706 ,\n",
       "        0.2509804 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
       "        0.50980395, 0.7176471 , 0.99215686, 0.99215686, 0.8117647 ,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15294118, 0.5803922 , 0.8980392 ,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.7137255 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09019608, 0.25882354,\n",
       "        0.8352941 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.7764706 , 0.31764707, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07058824, 0.67058825, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.7647059 , 0.3137255 ,\n",
       "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.21568628,\n",
       "        0.6745098 , 0.8862745 , 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.95686275, 0.52156866, 0.04313726, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.53333336,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.83137256, 0.5294118 ,\n",
       "        0.5176471 , 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Comprobamos la normalización\n",
    "'''\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos datos para validación. Estos datos se usarán durante el entrenamiento. Otra opción es decirle a keras en la etapa de entrenamiento que reserve un X % de los datos para validar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "\n",
    "X_train = X_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos la arquitectura de la red neuronal. Se va a componer de:\n",
    "* **Sequential**: API para iniciar la red neuronal. No cuenta como capa.\n",
    "* **Flatten**: capa de entrada. Necesita un vector unidimensional. Como tenemos imágenes, esta capa aplana las imagenes (2D) en 1D.\n",
    "* **Dense**: es una hidden layer. Se compondrá de `n` neuronas y de una función de activación que se aplicará a todas las neuronas de la capa.\n",
    "\n",
    "Recuerda que es un problema de clasificación multiclase (10 clases) y que por tanto la última capa se compondrá de tantas neuronas como clases tengas.\n",
    "\n",
    "En cuanto a las funciones de activación es recomendable usar relu en las hidden layer, que tarda menos en entrenar, mientras que la ultima (output) suele ser una softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "# Capa entrada\n",
    "model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "# Hidden layer\n",
    "model.add(keras.layers.Dense(units = 300,\n",
    "                            activation='relu'))\n",
    "\n",
    "# Hidden layer\n",
    "model.add(keras.layers.Dense(units = 100,\n",
    "                            activation='relu'))\n",
    "\n",
    "# Capa salida\n",
    "model.add(keras.layers.Dense(units = 10,\n",
    "                            activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otra manera de declarar la red neuronal\n",
    "capas = [\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(units = 300, activation='relu'),\n",
    "    keras.layers.Dense(units = 100, activation='relu'),\n",
    "    keras.layers.Dense(units = 10, activation='softmax')\n",
    "]\n",
    "\n",
    "model = keras.models.Sequential(capas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver las capas, y acceder a sus elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.reshaping.flatten.Flatten object at 0x00000227E48E4310>\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los pesos de las capas sin entrenar, porque los inicializa aleatoriamente. Los bias los inicializa a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1]\n",
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235200"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300*784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235200"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos la configuración de ejecución... el compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = keras.optimizers.SGD(),\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalente\n",
    "model.compile(\n",
    "    optimizer = \"sgd\",\n",
    "    loss = \"sparse_categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235500"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "784*300 + 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235500"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300*28*28 + 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo. Usamos los datos de entrenamiento. El batch_size es la cantidad de muestras que utiliza el SGD, y las epochs son las iteraciones que realiza en el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390.625"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50000/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390.625"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50000/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "391/391 [==============================] - 6s 13ms/step - loss: 1.2632 - accuracy: 0.6805 - val_loss: 0.6185 - val_accuracy: 0.8552\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.5353 - accuracy: 0.8609 - val_loss: 0.4051 - val_accuracy: 0.8932\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4085 - accuracy: 0.8872 - val_loss: 0.3424 - val_accuracy: 0.9039\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.3558 - accuracy: 0.8990 - val_loss: 0.3071 - val_accuracy: 0.9142\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3244 - accuracy: 0.9077 - val_loss: 0.2854 - val_accuracy: 0.9186\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3021 - accuracy: 0.9136 - val_loss: 0.2692 - val_accuracy: 0.9223\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2845 - accuracy: 0.9188 - val_loss: 0.2560 - val_accuracy: 0.9258\n",
      "Epoch 8/50\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2701 - accuracy: 0.9224 - val_loss: 0.2451 - val_accuracy: 0.9287\n",
      "Epoch 9/50\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2574 - accuracy: 0.9267 - val_loss: 0.2343 - val_accuracy: 0.9311\n",
      "Epoch 10/50\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2465 - accuracy: 0.9294 - val_loss: 0.2259 - val_accuracy: 0.9351\n",
      "Epoch 11/50\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2364 - accuracy: 0.9329 - val_loss: 0.2175 - val_accuracy: 0.9383\n",
      "Epoch 12/50\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2274 - accuracy: 0.9354 - val_loss: 0.2117 - val_accuracy: 0.9404\n",
      "Epoch 13/50\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2185 - accuracy: 0.9374 - val_loss: 0.2046 - val_accuracy: 0.9422\n",
      "Epoch 14/50\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2112 - accuracy: 0.9395 - val_loss: 0.1988 - val_accuracy: 0.9449\n",
      "Epoch 15/50\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2036 - accuracy: 0.9424 - val_loss: 0.1926 - val_accuracy: 0.9472\n",
      "Epoch 16/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1967 - accuracy: 0.9443 - val_loss: 0.1875 - val_accuracy: 0.9488\n",
      "Epoch 17/50\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1901 - accuracy: 0.9464 - val_loss: 0.1826 - val_accuracy: 0.9495\n",
      "Epoch 18/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1839 - accuracy: 0.9476 - val_loss: 0.1781 - val_accuracy: 0.9519\n",
      "Epoch 19/50\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1782 - accuracy: 0.9495 - val_loss: 0.1732 - val_accuracy: 0.9523\n",
      "Epoch 20/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1724 - accuracy: 0.9516 - val_loss: 0.1700 - val_accuracy: 0.9526\n",
      "Epoch 21/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1673 - accuracy: 0.9524 - val_loss: 0.1663 - val_accuracy: 0.9542\n",
      "Epoch 22/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1623 - accuracy: 0.9539 - val_loss: 0.1632 - val_accuracy: 0.9553\n",
      "Epoch 23/50\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1578 - accuracy: 0.9550 - val_loss: 0.1609 - val_accuracy: 0.9555\n",
      "Epoch 24/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1534 - accuracy: 0.9568 - val_loss: 0.1550 - val_accuracy: 0.9574\n",
      "Epoch 25/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1490 - accuracy: 0.9577 - val_loss: 0.1520 - val_accuracy: 0.9585\n",
      "Epoch 26/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1450 - accuracy: 0.9587 - val_loss: 0.1481 - val_accuracy: 0.9594\n",
      "Epoch 27/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1411 - accuracy: 0.9598 - val_loss: 0.1474 - val_accuracy: 0.9601\n",
      "Epoch 28/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1375 - accuracy: 0.9615 - val_loss: 0.1431 - val_accuracy: 0.9611\n",
      "Epoch 29/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1338 - accuracy: 0.9622 - val_loss: 0.1412 - val_accuracy: 0.9613\n",
      "Epoch 30/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1304 - accuracy: 0.9629 - val_loss: 0.1390 - val_accuracy: 0.9616\n",
      "Epoch 31/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.1271 - accuracy: 0.9641 - val_loss: 0.1356 - val_accuracy: 0.9632\n",
      "Epoch 32/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.1241 - accuracy: 0.9659 - val_loss: 0.1336 - val_accuracy: 0.9636\n",
      "Epoch 33/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1210 - accuracy: 0.9660 - val_loss: 0.1322 - val_accuracy: 0.9649\n",
      "Epoch 34/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.1181 - accuracy: 0.9672 - val_loss: 0.1300 - val_accuracy: 0.9635\n",
      "Epoch 35/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.1154 - accuracy: 0.9681 - val_loss: 0.1287 - val_accuracy: 0.9653\n",
      "Epoch 36/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.1126 - accuracy: 0.9686 - val_loss: 0.1261 - val_accuracy: 0.9651\n",
      "Epoch 37/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.1102 - accuracy: 0.9692 - val_loss: 0.1239 - val_accuracy: 0.9654\n",
      "Epoch 38/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1077 - accuracy: 0.9702 - val_loss: 0.1219 - val_accuracy: 0.9661\n",
      "Epoch 39/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1053 - accuracy: 0.9714 - val_loss: 0.1216 - val_accuracy: 0.9654\n",
      "Epoch 40/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1029 - accuracy: 0.9717 - val_loss: 0.1191 - val_accuracy: 0.9665\n",
      "Epoch 41/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1008 - accuracy: 0.9724 - val_loss: 0.1177 - val_accuracy: 0.9674\n",
      "Epoch 42/50\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.0986 - accuracy: 0.9730 - val_loss: 0.1164 - val_accuracy: 0.9673\n",
      "Epoch 43/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0966 - accuracy: 0.9734 - val_loss: 0.1156 - val_accuracy: 0.9678\n",
      "Epoch 44/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0944 - accuracy: 0.9743 - val_loss: 0.1137 - val_accuracy: 0.9684\n",
      "Epoch 45/50\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.0926 - accuracy: 0.9750 - val_loss: 0.1122 - val_accuracy: 0.9683\n",
      "Epoch 46/50\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.0907 - accuracy: 0.9752 - val_loss: 0.1100 - val_accuracy: 0.9688\n",
      "Epoch 47/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.0888 - accuracy: 0.9761 - val_loss: 0.1106 - val_accuracy: 0.9684\n",
      "Epoch 48/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0870 - accuracy: 0.9763 - val_loss: 0.1085 - val_accuracy: 0.9694\n",
      "Epoch 49/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0854 - accuracy: 0.9771 - val_loss: 0.1078 - val_accuracy: 0.9688\n",
      "Epoch 50/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.0835 - accuracy: 0.9779 - val_loss: 0.1064 - val_accuracy: 0.9698\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 128,\n",
    "    epochs = 50,\n",
    "    validation_data = (X_val, y_val) # validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos reentrenar el modelo. No empieza de nuevo, sino que retoma el entrenamiento anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0804 - accuracy: 0.9776 - val_loss: 0.1023 - val_accuracy: 0.9709\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.0777 - accuracy: 0.9785 - val_loss: 0.1002 - val_accuracy: 0.9723\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0749 - accuracy: 0.9795 - val_loss: 0.0973 - val_accuracy: 0.9720\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0721 - accuracy: 0.9810 - val_loss: 0.0979 - val_accuracy: 0.9718\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0697 - accuracy: 0.9810 - val_loss: 0.0953 - val_accuracy: 0.9728\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0671 - accuracy: 0.9821 - val_loss: 0.0979 - val_accuracy: 0.9710\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0650 - accuracy: 0.9826 - val_loss: 0.0931 - val_accuracy: 0.9727\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0629 - accuracy: 0.9831 - val_loss: 0.0920 - val_accuracy: 0.9731\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0605 - accuracy: 0.9839 - val_loss: 0.0909 - val_accuracy: 0.9742\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0585 - accuracy: 0.9842 - val_loss: 0.0911 - val_accuracy: 0.9733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x227cd2f0b20>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 64,\n",
    "    epochs = 10,\n",
    "    validation_data = (X_val, y_val) # validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el histórico del entrenamiento, para poder representarlo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': 1, 'epochs': 50, 'steps': 391}\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [1.15187668800354,\n",
       "  0.4944753646850586,\n",
       "  0.38587233424186707,\n",
       "  0.33907490968704224,\n",
       "  0.31059738993644714,\n",
       "  0.2898997962474823,\n",
       "  0.2730436623096466,\n",
       "  0.25927573442459106,\n",
       "  0.2471863329410553,\n",
       "  0.23607158660888672,\n",
       "  0.22634314000606537,\n",
       "  0.21738357841968536,\n",
       "  0.20898909866809845,\n",
       "  0.20119668543338776,\n",
       "  0.19404710829257965,\n",
       "  0.18709218502044678,\n",
       "  0.18088628351688385,\n",
       "  0.17489992082118988,\n",
       "  0.16950611770153046,\n",
       "  0.16419731080532074,\n",
       "  0.15935586392879486,\n",
       "  0.15444247424602509,\n",
       "  0.15031050145626068,\n",
       "  0.14593268930912018,\n",
       "  0.14226239919662476,\n",
       "  0.13833676278591156,\n",
       "  0.13479435443878174,\n",
       "  0.13129989802837372,\n",
       "  0.12796157598495483,\n",
       "  0.12478624284267426,\n",
       "  0.12179426848888397,\n",
       "  0.11892441660165787,\n",
       "  0.11609789729118347,\n",
       "  0.11345662921667099,\n",
       "  0.1108921468257904,\n",
       "  0.10836926847696304,\n",
       "  0.10599320381879807,\n",
       "  0.1035470962524414,\n",
       "  0.10132447630167007,\n",
       "  0.09926648437976837,\n",
       "  0.09711495041847229,\n",
       "  0.09503152221441269,\n",
       "  0.09315481781959534,\n",
       "  0.09129234403371811,\n",
       "  0.08944141864776611,\n",
       "  0.0875873938202858,\n",
       "  0.08584921061992645,\n",
       "  0.08408190310001373,\n",
       "  0.08258575201034546,\n",
       "  0.08092513680458069],\n",
       " 'accuracy': [0.7268400192260742,\n",
       "  0.8744999766349792,\n",
       "  0.8959000110626221,\n",
       "  0.9059600234031677,\n",
       "  0.9131399989128113,\n",
       "  0.9187399744987488,\n",
       "  0.9233999848365784,\n",
       "  0.9266200065612793,\n",
       "  0.9297400116920471,\n",
       "  0.9327399730682373,\n",
       "  0.9361000061035156,\n",
       "  0.9379400014877319,\n",
       "  0.9398000240325928,\n",
       "  0.9419199824333191,\n",
       "  0.944599986076355,\n",
       "  0.946179986000061,\n",
       "  0.9485200047492981,\n",
       "  0.9497799873352051,\n",
       "  0.9514200091362,\n",
       "  0.9530199766159058,\n",
       "  0.9546999931335449,\n",
       "  0.9564399719238281,\n",
       "  0.9572399854660034,\n",
       "  0.9581800103187561,\n",
       "  0.9595199823379517,\n",
       "  0.9603999853134155,\n",
       "  0.9614599943161011,\n",
       "  0.9624000191688538,\n",
       "  0.9631199836730957,\n",
       "  0.964739978313446,\n",
       "  0.9654200077056885,\n",
       "  0.966480016708374,\n",
       "  0.9673200249671936,\n",
       "  0.9683799743652344,\n",
       "  0.9690399765968323,\n",
       "  0.9697999954223633,\n",
       "  0.9699599742889404,\n",
       "  0.9708399772644043,\n",
       "  0.9716600179672241,\n",
       "  0.9721400141716003,\n",
       "  0.9730200171470642,\n",
       "  0.9736800193786621,\n",
       "  0.9741399884223938,\n",
       "  0.9746999740600586,\n",
       "  0.975059986114502,\n",
       "  0.9758599996566772,\n",
       "  0.9759200215339661,\n",
       "  0.9765400290489197,\n",
       "  0.9772800207138062,\n",
       "  0.9775000214576721],\n",
       " 'val_loss': [0.5634182095527649,\n",
       "  0.3825947642326355,\n",
       "  0.32674434781074524,\n",
       "  0.2967895269393921,\n",
       "  0.275500625371933,\n",
       "  0.2585633099079132,\n",
       "  0.2457362562417984,\n",
       "  0.23706835508346558,\n",
       "  0.22473178803920746,\n",
       "  0.21697553992271423,\n",
       "  0.20918312668800354,\n",
       "  0.20154187083244324,\n",
       "  0.19443219900131226,\n",
       "  0.18891817331314087,\n",
       "  0.18334715068340302,\n",
       "  0.17803731560707092,\n",
       "  0.17374593019485474,\n",
       "  0.1678672879934311,\n",
       "  0.16425712406635284,\n",
       "  0.16035674512386322,\n",
       "  0.15771575272083282,\n",
       "  0.15377940237522125,\n",
       "  0.1502869427204132,\n",
       "  0.14667552709579468,\n",
       "  0.14427436888217926,\n",
       "  0.14105363190174103,\n",
       "  0.13883261382579803,\n",
       "  0.13738003373146057,\n",
       "  0.1341695487499237,\n",
       "  0.13184261322021484,\n",
       "  0.13043402135372162,\n",
       "  0.12824954092502594,\n",
       "  0.12529638409614563,\n",
       "  0.1251932680606842,\n",
       "  0.12210091203451157,\n",
       "  0.12113159149885178,\n",
       "  0.11856500059366226,\n",
       "  0.11773256957530975,\n",
       "  0.11639892309904099,\n",
       "  0.11508463323116302,\n",
       "  0.11433469504117966,\n",
       "  0.11420372128486633,\n",
       "  0.11107155680656433,\n",
       "  0.11035127192735672,\n",
       "  0.10805915296077728,\n",
       "  0.10822713375091553,\n",
       "  0.10769327729940414,\n",
       "  0.1051434800028801,\n",
       "  0.10405024141073227,\n",
       "  0.10494417697191238],\n",
       " 'val_accuracy': [0.8701000213623047,\n",
       "  0.902899980545044,\n",
       "  0.9121999740600586,\n",
       "  0.9192000031471252,\n",
       "  0.9244999885559082,\n",
       "  0.9284999966621399,\n",
       "  0.9319999814033508,\n",
       "  0.9334999918937683,\n",
       "  0.9376999735832214,\n",
       "  0.9398999810218811,\n",
       "  0.9426000118255615,\n",
       "  0.9453999996185303,\n",
       "  0.9467999935150146,\n",
       "  0.9472000002861023,\n",
       "  0.9495999813079834,\n",
       "  0.951200008392334,\n",
       "  0.9513000249862671,\n",
       "  0.9544000029563904,\n",
       "  0.9546999931335449,\n",
       "  0.9555000066757202,\n",
       "  0.9573000073432922,\n",
       "  0.9574999809265137,\n",
       "  0.9595000147819519,\n",
       "  0.9595000147819519,\n",
       "  0.9602000117301941,\n",
       "  0.9614999890327454,\n",
       "  0.9613999724388123,\n",
       "  0.9611999988555908,\n",
       "  0.9628999829292297,\n",
       "  0.9631999731063843,\n",
       "  0.9639000296592712,\n",
       "  0.9642999768257141,\n",
       "  0.9646999835968018,\n",
       "  0.9648000001907349,\n",
       "  0.9653000235557556,\n",
       "  0.9653000235557556,\n",
       "  0.9666000008583069,\n",
       "  0.9661999940872192,\n",
       "  0.9675999879837036,\n",
       "  0.9671000242233276,\n",
       "  0.9678000211715698,\n",
       "  0.9684000015258789,\n",
       "  0.968999981880188,\n",
       "  0.968500018119812,\n",
       "  0.9693999886512756,\n",
       "  0.9692999720573425,\n",
       "  0.9696000218391418,\n",
       "  0.9699000120162964,\n",
       "  0.9714000225067139,\n",
       "  0.9704999923706055]}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(history.params)\n",
    "print(history.epoch)\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.15187668800354,\n",
       "  0.4944753646850586,\n",
       "  0.38587233424186707,\n",
       "  0.33907490968704224,\n",
       "  0.31059738993644714,\n",
       "  0.2898997962474823,\n",
       "  0.2730436623096466,\n",
       "  0.25927573442459106,\n",
       "  0.2471863329410553,\n",
       "  0.23607158660888672,\n",
       "  0.22634314000606537,\n",
       "  0.21738357841968536,\n",
       "  0.20898909866809845,\n",
       "  0.20119668543338776,\n",
       "  0.19404710829257965,\n",
       "  0.18709218502044678,\n",
       "  0.18088628351688385,\n",
       "  0.17489992082118988,\n",
       "  0.16950611770153046,\n",
       "  0.16419731080532074,\n",
       "  0.15935586392879486,\n",
       "  0.15444247424602509,\n",
       "  0.15031050145626068,\n",
       "  0.14593268930912018,\n",
       "  0.14226239919662476,\n",
       "  0.13833676278591156,\n",
       "  0.13479435443878174,\n",
       "  0.13129989802837372,\n",
       "  0.12796157598495483,\n",
       "  0.12478624284267426,\n",
       "  0.12179426848888397,\n",
       "  0.11892441660165787,\n",
       "  0.11609789729118347,\n",
       "  0.11345662921667099,\n",
       "  0.1108921468257904,\n",
       "  0.10836926847696304,\n",
       "  0.10599320381879807,\n",
       "  0.1035470962524414,\n",
       "  0.10132447630167007,\n",
       "  0.09926648437976837,\n",
       "  0.09711495041847229,\n",
       "  0.09503152221441269,\n",
       "  0.09315481781959534,\n",
       "  0.09129234403371811,\n",
       "  0.08944141864776611,\n",
       "  0.0875873938202858,\n",
       "  0.08584921061992645,\n",
       "  0.08408190310001373,\n",
       "  0.08258575201034546,\n",
       "  0.08092513680458069],\n",
       " 'accuracy': [0.7268400192260742,\n",
       "  0.8744999766349792,\n",
       "  0.8959000110626221,\n",
       "  0.9059600234031677,\n",
       "  0.9131399989128113,\n",
       "  0.9187399744987488,\n",
       "  0.9233999848365784,\n",
       "  0.9266200065612793,\n",
       "  0.9297400116920471,\n",
       "  0.9327399730682373,\n",
       "  0.9361000061035156,\n",
       "  0.9379400014877319,\n",
       "  0.9398000240325928,\n",
       "  0.9419199824333191,\n",
       "  0.944599986076355,\n",
       "  0.946179986000061,\n",
       "  0.9485200047492981,\n",
       "  0.9497799873352051,\n",
       "  0.9514200091362,\n",
       "  0.9530199766159058,\n",
       "  0.9546999931335449,\n",
       "  0.9564399719238281,\n",
       "  0.9572399854660034,\n",
       "  0.9581800103187561,\n",
       "  0.9595199823379517,\n",
       "  0.9603999853134155,\n",
       "  0.9614599943161011,\n",
       "  0.9624000191688538,\n",
       "  0.9631199836730957,\n",
       "  0.964739978313446,\n",
       "  0.9654200077056885,\n",
       "  0.966480016708374,\n",
       "  0.9673200249671936,\n",
       "  0.9683799743652344,\n",
       "  0.9690399765968323,\n",
       "  0.9697999954223633,\n",
       "  0.9699599742889404,\n",
       "  0.9708399772644043,\n",
       "  0.9716600179672241,\n",
       "  0.9721400141716003,\n",
       "  0.9730200171470642,\n",
       "  0.9736800193786621,\n",
       "  0.9741399884223938,\n",
       "  0.9746999740600586,\n",
       "  0.975059986114502,\n",
       "  0.9758599996566772,\n",
       "  0.9759200215339661,\n",
       "  0.9765400290489197,\n",
       "  0.9772800207138062,\n",
       "  0.9775000214576721],\n",
       " 'val_loss': [0.5634182095527649,\n",
       "  0.3825947642326355,\n",
       "  0.32674434781074524,\n",
       "  0.2967895269393921,\n",
       "  0.275500625371933,\n",
       "  0.2585633099079132,\n",
       "  0.2457362562417984,\n",
       "  0.23706835508346558,\n",
       "  0.22473178803920746,\n",
       "  0.21697553992271423,\n",
       "  0.20918312668800354,\n",
       "  0.20154187083244324,\n",
       "  0.19443219900131226,\n",
       "  0.18891817331314087,\n",
       "  0.18334715068340302,\n",
       "  0.17803731560707092,\n",
       "  0.17374593019485474,\n",
       "  0.1678672879934311,\n",
       "  0.16425712406635284,\n",
       "  0.16035674512386322,\n",
       "  0.15771575272083282,\n",
       "  0.15377940237522125,\n",
       "  0.1502869427204132,\n",
       "  0.14667552709579468,\n",
       "  0.14427436888217926,\n",
       "  0.14105363190174103,\n",
       "  0.13883261382579803,\n",
       "  0.13738003373146057,\n",
       "  0.1341695487499237,\n",
       "  0.13184261322021484,\n",
       "  0.13043402135372162,\n",
       "  0.12824954092502594,\n",
       "  0.12529638409614563,\n",
       "  0.1251932680606842,\n",
       "  0.12210091203451157,\n",
       "  0.12113159149885178,\n",
       "  0.11856500059366226,\n",
       "  0.11773256957530975,\n",
       "  0.11639892309904099,\n",
       "  0.11508463323116302,\n",
       "  0.11433469504117966,\n",
       "  0.11420372128486633,\n",
       "  0.11107155680656433,\n",
       "  0.11035127192735672,\n",
       "  0.10805915296077728,\n",
       "  0.10822713375091553,\n",
       "  0.10769327729940414,\n",
       "  0.1051434800028801,\n",
       "  0.10405024141073227,\n",
       "  0.10494417697191238],\n",
       " 'val_accuracy': [0.8701000213623047,\n",
       "  0.902899980545044,\n",
       "  0.9121999740600586,\n",
       "  0.9192000031471252,\n",
       "  0.9244999885559082,\n",
       "  0.9284999966621399,\n",
       "  0.9319999814033508,\n",
       "  0.9334999918937683,\n",
       "  0.9376999735832214,\n",
       "  0.9398999810218811,\n",
       "  0.9426000118255615,\n",
       "  0.9453999996185303,\n",
       "  0.9467999935150146,\n",
       "  0.9472000002861023,\n",
       "  0.9495999813079834,\n",
       "  0.951200008392334,\n",
       "  0.9513000249862671,\n",
       "  0.9544000029563904,\n",
       "  0.9546999931335449,\n",
       "  0.9555000066757202,\n",
       "  0.9573000073432922,\n",
       "  0.9574999809265137,\n",
       "  0.9595000147819519,\n",
       "  0.9595000147819519,\n",
       "  0.9602000117301941,\n",
       "  0.9614999890327454,\n",
       "  0.9613999724388123,\n",
       "  0.9611999988555908,\n",
       "  0.9628999829292297,\n",
       "  0.9631999731063843,\n",
       "  0.9639000296592712,\n",
       "  0.9642999768257141,\n",
       "  0.9646999835968018,\n",
       "  0.9648000001907349,\n",
       "  0.9653000235557556,\n",
       "  0.9653000235557556,\n",
       "  0.9666000008583069,\n",
       "  0.9661999940872192,\n",
       "  0.9675999879837036,\n",
       "  0.9671000242233276,\n",
       "  0.9678000211715698,\n",
       "  0.9684000015258789,\n",
       "  0.968999981880188,\n",
       "  0.968500018119812,\n",
       "  0.9693999886512756,\n",
       "  0.9692999720573425,\n",
       "  0.9696000218391418,\n",
       "  0.9699000120162964,\n",
       "  0.9714000225067139,\n",
       "  0.9704999923706055]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.151877</td>\n",
       "      <td>0.72684</td>\n",
       "      <td>0.563418</td>\n",
       "      <td>0.8701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.494475</td>\n",
       "      <td>0.87450</td>\n",
       "      <td>0.382595</td>\n",
       "      <td>0.9029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.385872</td>\n",
       "      <td>0.89590</td>\n",
       "      <td>0.326744</td>\n",
       "      <td>0.9122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.339075</td>\n",
       "      <td>0.90596</td>\n",
       "      <td>0.296790</td>\n",
       "      <td>0.9192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.310597</td>\n",
       "      <td>0.91314</td>\n",
       "      <td>0.275501</td>\n",
       "      <td>0.9245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.289900</td>\n",
       "      <td>0.91874</td>\n",
       "      <td>0.258563</td>\n",
       "      <td>0.9285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.273044</td>\n",
       "      <td>0.92340</td>\n",
       "      <td>0.245736</td>\n",
       "      <td>0.9320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.259276</td>\n",
       "      <td>0.92662</td>\n",
       "      <td>0.237068</td>\n",
       "      <td>0.9335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.247186</td>\n",
       "      <td>0.92974</td>\n",
       "      <td>0.224732</td>\n",
       "      <td>0.9377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.236072</td>\n",
       "      <td>0.93274</td>\n",
       "      <td>0.216976</td>\n",
       "      <td>0.9399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.226343</td>\n",
       "      <td>0.93610</td>\n",
       "      <td>0.209183</td>\n",
       "      <td>0.9426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.217384</td>\n",
       "      <td>0.93794</td>\n",
       "      <td>0.201542</td>\n",
       "      <td>0.9454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.208989</td>\n",
       "      <td>0.93980</td>\n",
       "      <td>0.194432</td>\n",
       "      <td>0.9468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.201197</td>\n",
       "      <td>0.94192</td>\n",
       "      <td>0.188918</td>\n",
       "      <td>0.9472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.194047</td>\n",
       "      <td>0.94460</td>\n",
       "      <td>0.183347</td>\n",
       "      <td>0.9496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.187092</td>\n",
       "      <td>0.94618</td>\n",
       "      <td>0.178037</td>\n",
       "      <td>0.9512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.180886</td>\n",
       "      <td>0.94852</td>\n",
       "      <td>0.173746</td>\n",
       "      <td>0.9513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.174900</td>\n",
       "      <td>0.94978</td>\n",
       "      <td>0.167867</td>\n",
       "      <td>0.9544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.169506</td>\n",
       "      <td>0.95142</td>\n",
       "      <td>0.164257</td>\n",
       "      <td>0.9547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.164197</td>\n",
       "      <td>0.95302</td>\n",
       "      <td>0.160357</td>\n",
       "      <td>0.9555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.159356</td>\n",
       "      <td>0.95470</td>\n",
       "      <td>0.157716</td>\n",
       "      <td>0.9573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.154442</td>\n",
       "      <td>0.95644</td>\n",
       "      <td>0.153779</td>\n",
       "      <td>0.9575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.150311</td>\n",
       "      <td>0.95724</td>\n",
       "      <td>0.150287</td>\n",
       "      <td>0.9595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.145933</td>\n",
       "      <td>0.95818</td>\n",
       "      <td>0.146676</td>\n",
       "      <td>0.9595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.142262</td>\n",
       "      <td>0.95952</td>\n",
       "      <td>0.144274</td>\n",
       "      <td>0.9602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.138337</td>\n",
       "      <td>0.96040</td>\n",
       "      <td>0.141054</td>\n",
       "      <td>0.9615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.134794</td>\n",
       "      <td>0.96146</td>\n",
       "      <td>0.138833</td>\n",
       "      <td>0.9614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.96240</td>\n",
       "      <td>0.137380</td>\n",
       "      <td>0.9612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.127962</td>\n",
       "      <td>0.96312</td>\n",
       "      <td>0.134170</td>\n",
       "      <td>0.9629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.124786</td>\n",
       "      <td>0.96474</td>\n",
       "      <td>0.131843</td>\n",
       "      <td>0.9632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.121794</td>\n",
       "      <td>0.96542</td>\n",
       "      <td>0.130434</td>\n",
       "      <td>0.9639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.118924</td>\n",
       "      <td>0.96648</td>\n",
       "      <td>0.128250</td>\n",
       "      <td>0.9643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.116098</td>\n",
       "      <td>0.96732</td>\n",
       "      <td>0.125296</td>\n",
       "      <td>0.9647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.113457</td>\n",
       "      <td>0.96838</td>\n",
       "      <td>0.125193</td>\n",
       "      <td>0.9648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.110892</td>\n",
       "      <td>0.96904</td>\n",
       "      <td>0.122101</td>\n",
       "      <td>0.9653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.108369</td>\n",
       "      <td>0.96980</td>\n",
       "      <td>0.121132</td>\n",
       "      <td>0.9653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.105993</td>\n",
       "      <td>0.96996</td>\n",
       "      <td>0.118565</td>\n",
       "      <td>0.9666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.103547</td>\n",
       "      <td>0.97084</td>\n",
       "      <td>0.117733</td>\n",
       "      <td>0.9662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.101324</td>\n",
       "      <td>0.97166</td>\n",
       "      <td>0.116399</td>\n",
       "      <td>0.9676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.099266</td>\n",
       "      <td>0.97214</td>\n",
       "      <td>0.115085</td>\n",
       "      <td>0.9671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.097115</td>\n",
       "      <td>0.97302</td>\n",
       "      <td>0.114335</td>\n",
       "      <td>0.9678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.095032</td>\n",
       "      <td>0.97368</td>\n",
       "      <td>0.114204</td>\n",
       "      <td>0.9684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.093155</td>\n",
       "      <td>0.97414</td>\n",
       "      <td>0.111072</td>\n",
       "      <td>0.9690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.091292</td>\n",
       "      <td>0.97470</td>\n",
       "      <td>0.110351</td>\n",
       "      <td>0.9685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.089441</td>\n",
       "      <td>0.97506</td>\n",
       "      <td>0.108059</td>\n",
       "      <td>0.9694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.087587</td>\n",
       "      <td>0.97586</td>\n",
       "      <td>0.108227</td>\n",
       "      <td>0.9693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.085849</td>\n",
       "      <td>0.97592</td>\n",
       "      <td>0.107693</td>\n",
       "      <td>0.9696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.084082</td>\n",
       "      <td>0.97654</td>\n",
       "      <td>0.105143</td>\n",
       "      <td>0.9699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.082586</td>\n",
       "      <td>0.97728</td>\n",
       "      <td>0.104050</td>\n",
       "      <td>0.9714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.080925</td>\n",
       "      <td>0.97750</td>\n",
       "      <td>0.104944</td>\n",
       "      <td>0.9705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   1.151877   0.72684  0.563418        0.8701\n",
       "1   0.494475   0.87450  0.382595        0.9029\n",
       "2   0.385872   0.89590  0.326744        0.9122\n",
       "3   0.339075   0.90596  0.296790        0.9192\n",
       "4   0.310597   0.91314  0.275501        0.9245\n",
       "5   0.289900   0.91874  0.258563        0.9285\n",
       "6   0.273044   0.92340  0.245736        0.9320\n",
       "7   0.259276   0.92662  0.237068        0.9335\n",
       "8   0.247186   0.92974  0.224732        0.9377\n",
       "9   0.236072   0.93274  0.216976        0.9399\n",
       "10  0.226343   0.93610  0.209183        0.9426\n",
       "11  0.217384   0.93794  0.201542        0.9454\n",
       "12  0.208989   0.93980  0.194432        0.9468\n",
       "13  0.201197   0.94192  0.188918        0.9472\n",
       "14  0.194047   0.94460  0.183347        0.9496\n",
       "15  0.187092   0.94618  0.178037        0.9512\n",
       "16  0.180886   0.94852  0.173746        0.9513\n",
       "17  0.174900   0.94978  0.167867        0.9544\n",
       "18  0.169506   0.95142  0.164257        0.9547\n",
       "19  0.164197   0.95302  0.160357        0.9555\n",
       "20  0.159356   0.95470  0.157716        0.9573\n",
       "21  0.154442   0.95644  0.153779        0.9575\n",
       "22  0.150311   0.95724  0.150287        0.9595\n",
       "23  0.145933   0.95818  0.146676        0.9595\n",
       "24  0.142262   0.95952  0.144274        0.9602\n",
       "25  0.138337   0.96040  0.141054        0.9615\n",
       "26  0.134794   0.96146  0.138833        0.9614\n",
       "27  0.131300   0.96240  0.137380        0.9612\n",
       "28  0.127962   0.96312  0.134170        0.9629\n",
       "29  0.124786   0.96474  0.131843        0.9632\n",
       "30  0.121794   0.96542  0.130434        0.9639\n",
       "31  0.118924   0.96648  0.128250        0.9643\n",
       "32  0.116098   0.96732  0.125296        0.9647\n",
       "33  0.113457   0.96838  0.125193        0.9648\n",
       "34  0.110892   0.96904  0.122101        0.9653\n",
       "35  0.108369   0.96980  0.121132        0.9653\n",
       "36  0.105993   0.96996  0.118565        0.9666\n",
       "37  0.103547   0.97084  0.117733        0.9662\n",
       "38  0.101324   0.97166  0.116399        0.9676\n",
       "39  0.099266   0.97214  0.115085        0.9671\n",
       "40  0.097115   0.97302  0.114335        0.9678\n",
       "41  0.095032   0.97368  0.114204        0.9684\n",
       "42  0.093155   0.97414  0.111072        0.9690\n",
       "43  0.091292   0.97470  0.110351        0.9685\n",
       "44  0.089441   0.97506  0.108059        0.9694\n",
       "45  0.087587   0.97586  0.108227        0.9693\n",
       "46  0.085849   0.97592  0.107693        0.9696\n",
       "47  0.084082   0.97654  0.105143        0.9699\n",
       "48  0.082586   0.97728  0.104050        0.9714\n",
       "49  0.080925   0.97750  0.104944        0.9705"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el modelo no ha ido bien, prueba a cambiar el learning rate, cambia de optimizador y después prueba a cambiar capas, neuronas y funciones de activación.\n",
    "\n",
    "Ya tenemos el modelo entrenado. Probémoslo con test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0908 - accuracy: 0.9735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09081213921308517, 0.9735000133514404]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cogemos el primero\n",
    "plt.imshow(X_test[0].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.32941177, 0.7254902 , 0.62352943, 0.5921569 ,\n",
       "         0.23529412, 0.14117648, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.87058824, 0.99607843, 0.99607843, 0.99607843,\n",
       "         0.99607843, 0.94509804, 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "         0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "         0.6666667 , 0.20392157, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.2627451 , 0.44705883, 0.28235295, 0.44705883,\n",
       "         0.6392157 , 0.8901961 , 0.99607843, 0.88235295, 0.99607843,\n",
       "         0.99607843, 0.99607843, 0.98039216, 0.8980392 , 0.99607843,\n",
       "         0.99607843, 0.54901963, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.06666667, 0.25882354, 0.05490196, 0.2627451 ,\n",
       "         0.2627451 , 0.2627451 , 0.23137255, 0.08235294, 0.9254902 ,\n",
       "         0.99607843, 0.41568628, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.3254902 , 0.99215686,\n",
       "         0.81960785, 0.07058824, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.08627451, 0.9137255 , 1.        ,\n",
       "         0.3254902 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.5058824 , 0.99607843, 0.93333334,\n",
       "         0.17254902, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.23137255, 0.9764706 , 0.99607843, 0.24313726,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.52156866, 0.99607843, 0.73333335, 0.01960784,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.03529412, 0.8039216 , 0.972549  , 0.22745098, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.49411765, 0.99607843, 0.7137255 , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.29411766,\n",
       "         0.9843137 , 0.9411765 , 0.22352941, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.07450981, 0.8666667 ,\n",
       "         0.99607843, 0.6509804 , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.01176471, 0.79607844, 0.99607843,\n",
       "         0.85882354, 0.13725491, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.14901961, 0.99607843, 0.99607843,\n",
       "         0.3019608 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.12156863, 0.8784314 , 0.99607843, 0.4509804 ,\n",
       "         0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.52156866, 0.99607843, 0.99607843, 0.20392157,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.23921569, 0.9490196 , 0.99607843, 0.99607843, 0.20392157,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4745098 , 0.99607843, 0.99607843, 0.85882354, 0.15686275,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4745098 , 0.99607843, 0.8117647 , 0.07058824, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 154ms/step\n",
      "(1, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.   , 0.   , 0.001, 0.   , 0.   , 0.   , 0.998, 0.   ,\n",
       "        0.   ]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test[:1]).round(3)\n",
    "print(predictions.shape)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=int64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[2].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema de regresión\n",
    "Veamos un ejemplo de cómo aplicar una red neuronal de TensorFlow a un problema de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#housing.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  \n",
       "2    -122.24   3.521  \n",
       "3    -122.25   3.413  \n",
       "4    -122.25   3.422  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos datos\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns = housing.feature_names)\n",
    "df['target'] = housing['target']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divimos en train, test y validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data,\n",
    "                                                              housing.target)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,\n",
    "                                                      y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.any(np.isnan(X_train)))\n",
    "print(np.any(np.isnan(X_valid)))\n",
    "print(np.any(np.isnan(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos el modelo. Simplemente se compondrá de una hidden layer, a la que le configuramos una capa previa de entrada de 8 neuronas (las features).\n",
    "\n",
    "Se trata de un modelo de regresión, por lo que la capa de salida es una única neurona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 2ms/step - loss: 1.0264 - val_loss: 0.5947\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6226 - val_loss: 0.5206\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5455 - val_loss: 0.4790\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4861 - val_loss: 0.4576\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4498 - val_loss: 0.4515\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4361 - val_loss: 0.4461\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4282 - val_loss: 0.4394\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4236 - val_loss: 0.4333\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4153 - val_loss: 0.4296\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4141 - val_loss: 0.4276\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4068 - val_loss: 0.4231\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4048 - val_loss: 0.4226\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4002 - val_loss: 0.4151\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3968 - val_loss: 0.4120\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3931 - val_loss: 0.4183\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3902 - val_loss: 0.4032\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3865 - val_loss: 0.4031\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3836 - val_loss: 0.4005\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3808 - val_loss: 0.3957\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3779 - val_loss: 0.3928\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = 'relu',\n",
    "                      input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.SGD(clipnorm=1)\n",
    "\n",
    "model.compile(loss = \"mean_squared_error\",\n",
    "             optimizer = optimizer) # con optimizer = \"sgd\" aparece la explosión del gradiente y nan's \n",
    "\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs = 20,\n",
    "                   validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8*30 + 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 30)                270       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 2ms/step - loss: 0.3669\n",
      "0.3669124245643616\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)\n",
    "print(mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.8753008],\n",
       "       [2.7724862],\n",
       "       [3.2521935],\n",
       "       [1.7324846],\n",
       "       [1.0945405]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test[:5])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelo\n",
    "Para guardar el modelo, en el formato de Keras (HDF5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo volvemos a cargar\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "Son funciones predefinidas de Keras a aplicar durante el entrenamiento\n",
    "Por ejemplo, `ModelCheckpoint` sirve para que el modelo se vaya guardando tras cada epoch. Así no perdemos el progreso en caso de que decidamos interrumpir el entrenamiento. El callback recibe como argumento el nombre del objeto donde queremos que se guarde el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3765\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3734\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3718\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3701\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3681\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3660\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3644\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3618\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3611\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3584\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3581\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3558\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3557\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3547\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3524\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3519\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3502\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3491\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3491\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3479\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3471\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3458\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3445\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3438\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3431\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3420\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3408\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3401\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3388\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3379\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"callback_model.h5\")\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs=30,\n",
    "                   callbacks = [checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Interrumpe el entrenamiento cuando no ve progreso en el set de validación. Para ello tiene en cuenta un numero de epochs llamado `patience`. Se puede combinar con el callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3366 - val_loss: 0.3556\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3356 - val_loss: 0.3625\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3352 - val_loss: 0.3562\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3349 - val_loss: 0.3583\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3341 - val_loss: 0.3520\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3328 - val_loss: 0.3556\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3327 - val_loss: 0.3512\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3327 - val_loss: 0.3579\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3324 - val_loss: 0.3559\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3309 - val_loss: 0.3505\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3303 - val_loss: 0.3482\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3288 - val_loss: 0.3528\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3286 - val_loss: 0.3525\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3284 - val_loss: 0.3479\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3276 - val_loss: 0.3523\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3276 - val_loss: 0.3486\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3271 - val_loss: 0.3461\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3262 - val_loss: 0.3519\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3262 - val_loss: 0.3521\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3256 - val_loss: 0.3547\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5)\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs=20,\n",
    "                    validation_data = (X_valid, y_valid),\n",
    "                   callbacks = [early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
